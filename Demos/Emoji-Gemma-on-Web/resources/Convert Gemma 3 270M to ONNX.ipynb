{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2025 Google LLC."
      ],
      "metadata": {
        "id": "BilmhAzR-4Qg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsslZVhvNrRy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Gemma 3 270M to ONNX\n",
        "\n",
        "This notebook converts a Gemma 3 model to the ONNX format for use with Transformers.js, allowing you to run models client-side in the browser.\n",
        "\n",
        "When training [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m) on a Colab T4 GPU accelerator, this takes under 10 minutes. Run each code snippet to:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Load and prepare Gemma 3 model from Hugging Face\n",
        "3. Convert the model with Optimum conversion script\n",
        "4. Test, evaluate, and save the model for further use\n",
        "\n",
        "Small models like Gemma 3 270M run efficiently on mobile, web, and edge devices and are designed for task-specific fine-tuning. This example converts and tests a model trained to translate text to emoji. To customize Gemma 3 270M models, run the fine-tuning notebook [here](https://).\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install the necessary libraries using pip."
      ],
      "metadata": {
        "id": "Cn2YgWGrNv2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.56.1 onnx==1.19.0 onnx_ir==0.1.7 onnxruntime==1.22.1 numpy==2.3.2 huggingface_hub"
      ],
      "metadata": {
        "id": "58TwFUM6SfFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to restart the runtime session to use newly installed packages."
      ],
      "metadata": {
        "id": "2g7wTSot_Erp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model\n",
        "Log in to Hugging Face with your [Access Token](https://huggingface.co/settings/tokens) by storing it as a Colab secret in the left toolbar. Specify `HF_TOKEN` as the 'Name' and add your unique token as the 'Value'."
      ],
      "metadata": {
        "id": "QuC7NbNy9JXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "6-fGnIinPjdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the model\n",
        "To convert the Gemma 3 Transformers model to ONNX, run the build_gemma.py script by [Xenova](https://huggingface.co/Xenova) that converts the Gemma 3 model into the ONNX format. First, download the script."
      ],
      "metadata": {
        "id": "L0PYnb5cODbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gist.githubusercontent.com/xenova/a219dbf3c7da7edd5dbb05f92410d7bd/raw/5791d43cc06bb11639bfbfdec32a2dd771313ffc/build_gemma.py\n"
      ],
      "metadata": {
        "id": "5EiWqZsUSvZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the conversion script, update:\n",
        "* `model_name` with the path to the model you want to convert\n",
        "* `output` with the name for your converted model.\n",
        "\n",
        "This should take under 5 minutes if you're using a Colab T4 GPU."
      ],
      "metadata": {
        "id": "8XpKMaKa6F-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_gemma.py \\\n",
        "    --model_name username/my-emojigemma \\\n",
        "    --output my-emojigemma-onnx/ \\\n",
        "    -p fp32 fp16 q4 q4f16"
      ],
      "metadata": {
        "id": "z292qdELOHOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the converted model\n",
        "\n",
        "After the exported .onnx model(s) have saved to your Colab session, try testing inference using ONNX Runtime.\n",
        "\n",
        "Test different text inputs in `text_to_translate` and try out different quantized versions, such as model_q4.onnx or model_q4f16.onnx, that are now in the /onnx/ folder."
      ],
      "metadata": {
        "id": "TB5dTmCXWQgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, GenerationConfig\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# Load config, processor, and model\n",
        "local_model_path = \"/content/my-emojigemma-onnx/\"\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_path = \"/content/my-emojigemma-onnx/onnx/model.onnx\"\n",
        "decoder_session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Prepare inputs\n",
        "text_to_translate = \"i love sushi\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"Translate this text to emoji.\" },\n",
        "  { \"role\": \"user\", \"content\": text_to_translate },\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "batch_size = input_ids.shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "position_ids = np.tile(np.arange(0, input_ids.shape[-1]), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 8\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids, dtype=np.int64)], axis=-1)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "  if np.isin(input_ids, eos_token_id).any():\n",
        "    break\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz8E9RorWWVn",
        "outputId": "ea3f48e7-da64-495d-983b-df0be95e35db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç£ü•¢üçôüçöüáØüáµ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Hugging Face Hub\n",
        "\n",
        "If you're happy with your model, you can upload it to [Hugging Face](https://huggingface.co/) for easy sharing and use."
      ],
      "metadata": {
        "id": "7wVBEPJKWSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "\n",
        "# The local folder in your Colab session that you want to upload\n",
        "local_model_path = \"/content/my-emojigemma-onnx\"\n",
        "\n",
        "# The name you want for your new repository on the Hugging Face Hub\n",
        "hf_username = \"username\"      #@param {type:\"string\"}\n",
        "repo_name = \"repo name\"       #@param {type:\"string\"}\n",
        "hf_repo_id = f\"{hf_username}/{repo_name}\"\n",
        "\n",
        "huggingface_hub.create_repo(hf_repo_id, exist_ok=True)\n",
        "\n",
        "repo_url = huggingface_hub.upload_folder(\n",
        "  folder_path=local_model_path,\n",
        "  repo_id=hf_repo_id,\n",
        "  repo_type=\"model\",\n",
        "  commit_message=f\"Upload ONNX model files for {repo_name}\"\n",
        "  )\n",
        "\n",
        "print(f\"Uploaded to {repo_url}\")"
      ],
      "metadata": {
        "id": "b4FNJTpKgT7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model with ONNX Runtime\n",
        "\n",
        "You can now run inference with your model using ONNX Runtime (ORT) which supports deployment on a wide range of platforms and operating systems.\n",
        "\n",
        "This means you can use [Transformers.js](https://huggingface.co/docs/transformers.js/en/index) to run .onnx models directly in the browser. Try out your model in a demo web app."
      ],
      "metadata": {
        "id": "cDEBKuXI_V--"
      }
    }
  ]
}