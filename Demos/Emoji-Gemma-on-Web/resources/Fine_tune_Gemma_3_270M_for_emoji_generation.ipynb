{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926bada6"
      },
      "source": [
        "Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a110dfce"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e624ec07"
      },
      "source": [
        "# Fine-tune Gemma 3 270M for emoji generation\n",
        "\n",
        "This notebook fine-tunes Gemma for the task of translating text into emoji using Quantized Low-Rank Adaptation (QLoRA) through the Hugging Face Transformers library to help reduce memory usage and speed up the fine-tuning process.\n",
        "\n",
        "When training [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m) on a Google Colab T4 GPU accelerator, this process can take as little as 10 minutes end-to-end. Run each code snippet to:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Prepare a dataset for fine-tuning\n",
        "3. Load and test the base Gemma 3 270M model\n",
        "4. Fine-tune the model\n",
        "5. Test, evaluate, and save the model for further use\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer, which handles dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BEK9IfKBqQaA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b1b70a6-b9dc-4068-cd66-a6c3c4be45fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, protobuf, transformers, datasets, bitsandbytes, trl, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 datasets-4.1.1 evaluate-0.4.6 protobuf-3.20.3 pyarrow-21.0.0 transformers-4.56.2 trl-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "02b272598e7d47d488b001a523020bf8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install torch tensorboard emoji\n",
        "%pip install -U transformers trl datasets accelerate evaluate sentencepiece bitsandbytes protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have the restart your session (runtime) to use newly installed libraries."
      ],
      "metadata": {
        "id": "TTuW1LPfLXi9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef3d54b"
      },
      "source": [
        "##Enable Hugging Face permissions\n",
        "To start using Gemma models, you'll need a Hugging Face account that has accepted the model usage license and and created an Access Token.\n",
        "\n",
        "1. **Accept usage license** on the [model page](http://huggingface.co/google/gemma-3-270m-it).\n",
        "\n",
        "2. **Get a valid [Access Token](https://huggingface.co/settings/tokens) with 'Write' access (very important!)** and create a new secret in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b6d79c93"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c60525"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "You can customize the Gemma 3 270M model to use specific emoji by creating a .csv spreadsheet containing your text-to-emoji dataset structured as key-value pairs. To encourage memorization of specific emoji, provide at least 20 examples in your dataset.\n",
        "\n",
        "Use our [premade dataset](https://github.com/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Emoji%20Translation%20Dataset%20%20-%20Dataset.csv) or this [spreadsheet](https://github.com/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Emoji%20Translation%20Dataset%20Template%20-%20Dataset.csv) as a template to create your own dataset, then upload it to session storage in the Files folder in the left toolbar. Get its path by right-clicking the file and pointing to it in `custom_dataset_path`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "custom_dataset_path = \"/content/Emoji Translation Dataset - Dataset.csv\"      # Point to your uploaded dataset\n",
        "dataset = load_dataset(\"csv\", data_files=custom_dataset_path, encoding=\"utf-8\", split=\"train\")\n",
        "\n",
        "print(f\"Here's the 10th example from your custom dataset: {dataset[10]}\")"
      ],
      "metadata": {
        "id": "g0HGgZj_3f7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6533212-849d-47cc-a7d8-027f78e06a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the 10th example from your custom dataset: {'text': 'A crisp and cool autumn morning', 'emoji': 'üçÇüçÅ‚òïÔ∏èüß°'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use a Hugging Face dataset\n",
        "**If you've already loaded a dataset, skip this step.**\n",
        "\n",
        "Hugging Face has a vast collection of datasets used to train and evaluate large language models (LLM). For the text-to-emoji task, you can use the [KomeijiForce/Text2Emoji](https://huggingface.co/datasets/KomeijiForce/Text2Emoji) dataset containing examples of text and corresponding emoji translations.\n",
        "\n",
        "As a preprocessing step, we'll use the [emoji](https://carpedm20.github.io/emoji/docs/) library to check that the training dataset \"outputs\" contain only emoji characters."
      ],
      "metadata": {
        "id": "kc1rhhdFGwNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc3BYl72pWhp"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Use the first 2000 samples for efficient training.\n",
        "general_dataset_path = load_dataset(\"KomeijiForce/Text2Emoji\", encoding=\"utf-8\", split=\"train[:2000]\")\n",
        "\n",
        "# Clean dataset to only use examples where 'emoji' field contains only emoji characters\n",
        "def is_only_emoji(sample):\n",
        "  emoji_string = sample['emoji']\n",
        "  if not emoji_string:\n",
        "    return False\n",
        "  return all(emoji.is_emoji(char) for char in emoji_string)\n",
        "dataset = general_dataset_path.filter(is_only_emoji)\n",
        "\n",
        "print(f\"\\nHere's the 10th example from the dataset: {dataset[10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format the training dataset\n",
        "Now that you've loaded your data, formatting it in conversational roles helps the model learn how to interpret the 'text' and 'emoji' columns from your dataset alongside a system prompt that contains the task instruction."
      ],
      "metadata": {
        "id": "PafivP8u1Gv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def translate(sample):\n",
        "  return {\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "          {\"role\": \"user\", \"content\": f\"{sample['text']}\"},\n",
        "          {\"role\": \"assistant\", \"content\": f\"{sample['emoji']}\"}\n",
        "      ]\n",
        "  }\n",
        "\n",
        "training_dataset = dataset.map(translate, remove_columns=dataset.features.keys())\n",
        "training_dataset_splits = training_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
        "\n",
        "print(\"\\nHere's an example from the formatted training dataset:\")\n",
        "print(training_dataset[50])"
      ],
      "metadata": {
        "id": "VWz32s5h074E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "5e7c715122c24da280c38558bfba5c57",
            "a1e53e5e598c4f99895b8c8a2fcf8251",
            "53f1d086681d4912b7f6554620949c9a",
            "b652203c39d242c98e32e73f7e32d6c9",
            "9b161b51c834455481b9739736712867",
            "b0eae0a1be154df594c1803c158d9a92",
            "ad658a15b1f4411c85644899aa60560e",
            "1e1566a1e1024d93903e817fed651c28",
            "f78775e9f1fe44edb45dcd9eea9b2995",
            "68955e79336540b08cc5cb11e65cb808",
            "2756889aea534fb38865e70114fb4659"
          ]
        },
        "outputId": "73802b8a-64b3-492f-9d6d-feb423a9ebee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2530 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e7c715122c24da280c38558bfba5c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Here's an example from the formatted training dataset:\n",
            "{'messages': [{'content': 'Translate this text to emoji: ', 'role': 'system'}, {'content': 'Always fishing for compliments', 'role': 'user'}, {'content': 'üé£üôÑüòí', 'role': 'assistant'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0eb2e06"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "You can access the [Gemma 3 270M model](https://huggingface.co/google/gemma-3-270m-it) from Hugging Face Hub by accepting the license terms.\n",
        "\n",
        "The instruction-tuned version of the model has already been trained on how to follow directions, enabling fine-tuning with fewer training examples and in less time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18069ed2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\", attn_implementation=\"eager\", dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "print(f\"Device: {base_model.device}\")\n",
        "print(f\"DType: {base_model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map special emoji characters\n",
        "\n",
        "Gemma 3 270M features a 256k token vocabulary, enabling it to handle specific and rare tokens. Some emojis aren't part of Gemma's base vocabulary, so this step ensures the emojis in your dataset are mapped to one of the model tokenizer's `<unused>` placeholder tokens before fine-tuning."
      ],
      "metadata": {
        "id": "new_markdown_cell_for_mapping"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify special tokens in your data that are not in the base vocabulary\n",
        "unk_token_id = tokenizer.unk_token_id\n",
        "unique_chars_in_data = {char for emoji_string in dataset['emoji'] if emoji_string for char in emoji_string}\n",
        "special_tokens_in_data = {char for char in unique_chars_in_data if tokenizer.convert_tokens_to_ids(char) == unk_token_id}\n",
        "\n",
        "# Get the list of available unused tokens that are already in the vocabulary\n",
        "available_placeholders = [token for token in tokenizer.get_vocab().keys() if token.startswith(\"<unused\")]\n",
        "if len(special_tokens_in_data) > len(available_placeholders):\n",
        "    raise ValueError(\"Not enough unused tokens to map all special tokens in the dataset.\")\n",
        "\n",
        "# Create unused token mapping\n",
        "token_map = {special: unused for special, unused in zip(special_tokens_in_data, available_placeholders)}\n",
        "def replace_special_tokens(example):\n",
        "    if example['emoji']:\n",
        "        example['emoji'] = \"\".join(token_map.get(char, char) for char in example['emoji'])\n",
        "    return example\n",
        "\n",
        "print(\"Mapped {len(special_tokens_in_data)} special token(s):\", token_map)"
      ],
      "metadata": {
        "id": "new_code_cell_for_mapping"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device should print as `cuda` if you're using a GPU runtime. **If you haven't aleady, use a free T4 GPU runtime in your Colab for faster fine-tuning.**"
      ],
      "metadata": {
        "id": "7hI4twbrz0xj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3w3b9-O4fDz"
      },
      "source": [
        "### Recommended: Test the base model\n",
        "\n",
        "Let's first check how the base model's ability to respond to the instruction \"Translate this text to emoji\".\n",
        "\n",
        "Try testing it a few times."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "# Select a random sample from the test dataset\n",
        "rand_idx = randint(0, len(training_dataset_splits[\"test\"]) - 1)\n",
        "test_sample = training_dataset_splits[\"test\"][rand_idx]\n",
        "\n",
        "# Handle messages\n",
        "all_messages = test_sample['messages']\n",
        "user_message_content = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'user'), \"Not Found\")\n",
        "dataset_emoji = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'assistant'), \"Not Found\")\n",
        "prompt_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "    {\"role\": \"user\", \"content\": user_message_content}\n",
        "]\n",
        "\n",
        "# Apply the chat template. This will format the messages correctly for the model.\n",
        "prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Generate the output\n",
        "output = pipe(prompt, max_new_tokens=64)\n",
        "model_output_only = output[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\nDataset text: {user_message_content}\")\n",
        "print(f\"\\nDataset emoji: {dataset_emoji}\")\n",
        "print(f\"\\nModel generated output: {model_output_only}\")"
      ],
      "metadata": {
        "id": "u8L0_INJyUok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You'll see that the base model does not generate a good emoji combination.** It might not output emoji within its response at all!\n",
        "\n",
        "That's because the Gemma 3 270M model size was designed for task hyper-specialization, which means it can improve performance for specific use cases when fine-tuned with representative examples. While the instruction-tuned model's previous training makes the translation task easier to learn, it's best to provide as many  examples in your dataset as possible.\n",
        "\n",
        "Now, you'll train the model with new data so it more reliably generates emoji."
      ],
      "metadata": {
        "id": "ph26HDJgua3W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd9fc1b"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Transformer Reinforcement Learning ([TRL](https://huggingface.co/docs/trl/index)) is a library that provides tools for training and fine-tuning LLMs using advanced techniques like QLoRA (Quantized Low-Rank Adaptation) to train adapters on top of a frozen quantized version of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the tuning job\n",
        "Define the training hyperparameters for the Gemma 3 base model:\n",
        "\n",
        "1. `BitsandBytesConfig` to quantize the model for memory efficiency\n",
        "2. `LoraConfig` for parameter-efficient fine-tuning\n",
        "2. `SFTConfig` for supervised fine-tuning\n",
        "\n",
        "QLoRA fine-tuning produces a set of adapters that we'll save to the Colab session storage so we can test model performance."
      ],
      "metadata": {
        "id": "-BJFoOdL0y8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTConfig\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"\n",
        "adapter_path = \"/content/myemoji-gemma-adapters\"      # Where to save your LoRA adapters\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=\"all-linear\",                      # Target all linear layers\n",
        "    lora_dropout=0.05,                                # Increase to 0.1 to induce overfitting\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\"lm_head\", \"embed_tokens\"]       # Save the lm_head and embed_tokens as you train the special tokens\n",
        ")\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=adapter_path,                          # Directory to save adapters\n",
        "    num_train_epochs=2,                               # Number of training epochs\n",
        "    per_device_train_batch_size=4,                    # Batch size per device during training\n",
        "    logging_strategy=\"epoch\",                         # Log every epoch\n",
        "    eval_strategy=\"epoch\",                            # Evaluate loss metrics every epoch\n",
        "    save_strategy=\"epoch\",                            # Save checkpoint every epoch\n",
        "    learning_rate=5e-5,                               # Learning rate\n",
        "    lr_scheduler_type=\"constant\",                     # Use constant learning rate scheduler\n",
        "    max_length=256,                                   # Max sequence length for model and packing of the dataset\n",
        "    gradient_checkpointing=False,                     # Use gradient checkpointing to save memory\n",
        "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
        "    optim=\"adamw_torch_fused\",                        # use fused adamw optimizer\n",
        "    report_to=\"tensorboard\",                          # Report metrics to tensorboard\n",
        "    weight_decay=0.01,                                # Added weight decay for regularization\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, quantization_config=bnb_config, device_map=\"auto\", attn_implementation='eager')\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Training configured.\")"
      ],
      "metadata": {
        "id": "qiIj1ADc-exw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd88e798"
      },
      "source": [
        "### Start training\n",
        "\n",
        "`SFTTrainer` tokenizes the training and evaluation datasets and trains the base model using the parameters from the previous step.\n",
        "\n",
        "**This shouldn't take more than 5 minutes** using GPU acceleration for 500 training examples. If your training is going slowly, check that you're using a T4 GPU in Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# Set the training and evaluation datasets\n",
        "train_dataset = training_dataset_splits['train']\n",
        "eval_dataset = training_dataset_splits['test']\n",
        "\n",
        "# Fine-tune the model and save the LoRA adapters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(adapter_path)\n",
        "\n",
        "print(f\"LoRA adapters saved to {adapter_path}\")"
      ],
      "metadata": {
        "id": "WqacJNeU9v7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LoRA adapters for each training checkpoint (epoch) will be saved in your temporary Colab session storage. You can choose which adapters to merge into the model. But first, take a look at the results."
      ],
      "metadata": {
        "id": "dDvGlb5xO34z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll8zZ3_u8Mt"
      },
      "source": [
        "### Plot training results\n",
        "To evaluate the model, you can plot the training and validation losses. Libraries like Matplotlib can then be used to visualize these values over training steps or epochs. This visualization helps in monitoring the training process and making informed decisions about hyperparameters tuning or early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPN-DTopaUIy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "3ebda88b-1f5c-4841-b953-06123a2c3279"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyIwS-orvWzd"
      },
      "source": [
        "Training loss measures the error on the data the model was trained on. Validation loss measures the error on a separate dataset the model has not seen before. Monitoring both helps detect overfitting (when the model performs well on training data but poorly on unseen data).\n",
        "\n",
        "- validation loss >> training loss: **overfitting**\n",
        "- validation loss > training loss: **some overfitting**\n",
        "- validation loss < training loss: **some underfitting**\n",
        "- validation loss << training loss: **underfitting**\n",
        "\n",
        "Overfitting is when the model can make accurate predictions on examples seen in its training but performs poorly with unseen data. If your task requires memory of specific examples, or specific emoji to be generated for a given text, overfitting can be a good thing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge the adapters\n",
        "\n",
        "Once trained you can merge the LoRA adapters with the model. You can choose which adapters to merge by specifying the training checkpoint folder, otherwise it will default to the last epoch.\n",
        "* For better task generalization, choose the most underfit checkpoint (validation loss < training loss)\n",
        "* For better memorization of specific examples, choose the most overfit (checkpoint > training loss)  \n",
        "\n"
      ],
      "metadata": {
        "id": "ILgEZvZ71Edz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"                            # Base model to merge from Hugging Face Hub\n",
        "adapter_path = \"/content/myemoji-gemma-adapters/\"                 # Choose which adapters to merge, otherwise defaults to latest\n",
        "merged_model_path = \"/content/myemoji-gemma-merged/\"              # Location of merged model directory\n",
        "\n",
        "# Load base model and tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "\n",
        "# Load and merge the PEFT adapters onto the base model\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model and its tokenizer\n",
        "model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "print(f\"Model merged and saved to {merged_model_path}. Final model vocabulary size: {model.config.vocab_size}\")"
      ],
      "metadata": {
        "id": "A7e5BQ9U06Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary size of the fine-tuned model should be the same as the base model (262144) to ensure compatibility for different runtimes."
      ],
      "metadata": {
        "id": "jAn8LI6dxnSV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf86e31d"
      },
      "source": [
        "### Test the fine-tuned model\n",
        "\n",
        "After the training is done and you've merged the adapters to the base model, you can test different user inputs for your fine-tuned model by updating `text_to_translate`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Create Transformers inference pipeline\n",
        "merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
        "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
        "\n",
        "# Test a prompt\n",
        "text_to_translate = \"let's go to Japan\"  #@param {type:\"string\"}\n",
        "inference_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "    {\"role\": \"user\", \"content\": text_to_translate}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(inference_messages, tokenize=False, add_generation_prompt=True)\n",
        "output = pipe(prompt, max_new_tokens=64)\n",
        "model_output = output[0]['generated_text']\n",
        "\n",
        "print(f\"{model_output}\")"
      ],
      "metadata": {
        "id": "28R3pRN_hai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the model output the emoji you'd expect?\n",
        "\n",
        "If you're not getting the results you want, you can try [retraining the model](#scrollTo=-BJFoOdL0y8w) using different parameters, or updating your training dataset to contain more representative examples.\n",
        "\n",
        "Once you're happy with the results, you can save your model to Hugging Face Hub for easy access."
      ],
      "metadata": {
        "id": "86qPcFbHH_kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save your model and upload to Hugging Face Hub\n",
        "**You now have a customized Gemma 3 270M model! üéâ**\n",
        "\n",
        "You can now download the merged model (or just your LoRA adapters) to the Download folders on your local machine, or upload to a repository on Hugging Face so you easily share your model or access it later for conversion."
      ],
      "metadata": {
        "id": "H12D9g4X_peV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name your fine-tuned model\n",
        "model_name = \"myemoji\"      #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "SDenDlxGMA-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload to Hugging Face\n",
        "Upload your merged model to [Hugging Face](https://huggingface.co/) for easy sharing and future use."
      ],
      "metadata": {
        "id": "mLFJWnH3K48a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami, HfApi\n",
        "from peft import PeftModel\n",
        "\n",
        "user_info = whoami()\n",
        "username = user_info['name']\n",
        "repo_id = f\"{username}/{model_name}-gemma-3-270m-it\"\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, exist_ok=True)\n",
        "api.upload_folder(folder_path=merged_model_path, repo_id=repo_id, repo_type=\"model\")"
      ],
      "metadata": {
        "id": "VbeyDcpwi4IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8ff452"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This notebook covered how to perform QLoRA fine-tuning on Gemma 3 270M using the Transformers TRL library. Now you can use your customized model in apps!\n",
        "\n",
        "### Run your model on-device\n",
        "\n",
        "To run your fine-tuned model on-device, continue on to the conversion and quantization steps. To run your emoji generation model directly in the browser, you can follow the steps to either:\n",
        "\n",
        "1.  [Convert for use with MediaPipe LLM Inference API](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)\n",
        "2.  [Convert for use with Transformers.js via ONNX Runtime](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)\n",
        "\n",
        "> Tip: When converting or running inference with your fine-tuned model, you'll get the best performance by using the same chat template it was trained on:\n",
        "> ```\n",
        "> <bos><start_of_turn>user\n",
        "> Translate this text to emoji.\n",
        ">\n",
        "> {input}<end_of_turn>\n",
        "> <start_of_turn>model\n",
        "> {output}\n",
        "> ```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "collapsed_sections": [
        "kc1rhhdFGwNu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}