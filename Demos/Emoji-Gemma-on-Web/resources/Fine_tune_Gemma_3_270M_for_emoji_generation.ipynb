{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926bada6"
      },
      "source": [
        "Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a110dfce"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e624ec07"
      },
      "source": [
        "# Fine-tune Gemma 3 270M for emoji generation\n",
        "\n",
        "This notebook fine-tunes Gemma for the task of translating text into emoji using Quantized Low-Rank Adaptation (QLoRA) through the Hugging Face Transformers library for memory effiency.\n",
        "\n",
        "When fine-tuning [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m) using QLoRA with T4 GPU acceleration in Colab, the entire process can take as little as 10 minutes end-to-end. Run each code snippet to:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Prepare a dataset for fine-tuning\n",
        "3. Load and test the base Gemma 3 270M model\n",
        "4. Fine-tune the model\n",
        "5. Test, evaluate, and save the model for further use\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer, which handles dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEK9IfKBqQaA"
      },
      "outputs": [],
      "source": [
        "%pip install torch tensorboard emoji\n",
        "%pip install -U transformers trl datasets accelerate evaluate sentencepiece bitsandbytes protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have the restart your session (runtime) to use newly installed libraries."
      ],
      "metadata": {
        "id": "TTuW1LPfLXi9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef3d54b"
      },
      "source": [
        "##Enable Hugging Face permissions\n",
        "To start using Gemma models, you'll need a Hugging Face account that has accepted the model usage license and and created an Access Token.\n",
        "\n",
        "1. **Accept usage license** on the [model page](http://huggingface.co/google/gemma-3-270m-it).\n",
        "\n",
        "2. **Get a valid [Access Token](https://huggingface.co/settings/tokens) with 'Write' access (very important!)** and create a new secret in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6d79c93"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c60525"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "You can customize the Gemma 3 270M model to use specific emoji by creating a .csv spreadsheet containing your text-to-emoji dataset structured as key-value pairs. To encourage memorization of specific emoji, provide at least 20 examples in your dataset.\n",
        "\n",
        "Use our [premade dataset](https://github.com/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Emoji%20Translation%20Dataset%20%20-%20Dataset.csv) or this [spreadsheet](https://github.com/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Emoji%20Translation%20Dataset%20Template%20-%20Dataset.csv) as a template to create your own dataset, then upload it to session storage in the Files folder in the left toolbar. Get its path by right-clicking the file and pointing to it in `custom_dataset_path`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "custom_dataset_path = \"/content/Emoji Translation Dataset - Dataset.csv\"      # Upload and point to your dataset\n",
        "dataset = load_dataset(\"csv\", data_files=custom_dataset_path, encoding=\"utf-8\", split=\"train\")\n",
        "\n",
        "print(f\"Here's the 10th example from your custom dataset: {dataset[10]}\")"
      ],
      "metadata": {
        "id": "g0HGgZj_3f7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use a Hugging Face dataset\n",
        "**If you've already loaded a dataset, skip this step.**\n",
        "\n",
        "Hugging Face has a vast collection of datasets used to train and evaluate large language models (LLM). For the text-to-emoji task, you can use the [KomeijiForce/Text2Emoji](https://huggingface.co/datasets/KomeijiForce/Text2Emoji) dataset containing examples of text and corresponding emoji translations."
      ],
      "metadata": {
        "id": "kc1rhhdFGwNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc3BYl72pWhp"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Use the first 2000 samples for efficient training.\n",
        "general_dataset_path = load_dataset(\"KomeijiForce/Text2Emoji\", encoding=\"utf-8\", split=\"train[:2000]\")\n",
        "\n",
        "# Clean dataset to only use examples where 'emoji' field contains only emoji characters\n",
        "def is_only_emoji(sample):\n",
        "  emoji_string = sample['emoji']\n",
        "  if not emoji_string:\n",
        "    return False\n",
        "  return all(emoji.is_emoji(char) for char in emoji_string)\n",
        "dataset = general_dataset_path.filter(is_only_emoji)\n",
        "\n",
        "print(f\"\\nHere's the 10th example from the dataset: {dataset[10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format the training dataset\n",
        "Now that you've loaded your data, format the training dataset into conversational roles, including the text input, emoji output, and a system prompt that contains the direction to the model. This helps the model learn how to interpret the 'text' and 'emoji' columns from your dataset."
      ],
      "metadata": {
        "id": "PafivP8u1Gv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def translate(sample):\n",
        "  return {\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "          {\"role\": \"user\", \"content\": f\"{sample['text']}\"},\n",
        "          {\"role\": \"assistant\", \"content\": f\"{sample['emoji']}\"}\n",
        "      ]\n",
        "  }\n",
        "\n",
        "training_dataset = dataset.map(translate, remove_columns=dataset.features.keys())\n",
        "training_dataset_splits = training_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
        "\n",
        "print(\"Here's an example from the formatted training dataset:\")\n",
        "print(training_dataset[50])"
      ],
      "metadata": {
        "id": "VWz32s5h074E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0eb2e06"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "You can access the [Gemma 3 270M model](https://huggingface.co/google/gemma-3-270m-it) from Hugging Face Hub by accepting the license terms.\n",
        "\n",
        "The instruction-tuned version of the model has already been trained on how to follow directions, enabling fine-tuning with fewer training examples and in less time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18069ed2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\", attn_implementation=\"eager\", dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "print(f\"Device: {base_model.device}\")\n",
        "print(f\"DType: {base_model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device should print as `cuda` if you're using a GPU runtime. **If you haven't aleady, use a free T4 GPU runtime in your Colab for faster fine-tuning.**"
      ],
      "metadata": {
        "id": "7hI4twbrz0xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map special characters\n",
        "\n",
        "Gemma 3 270M was pre-trained with over 140 languages and features a large 256k token vocabulary, enabling it to handle specific and rare tokens.\n",
        "\n",
        "Some emojis aren't part of Gemma's base vocabulary, so this step ensures the emojis in your dataset are mapped to one of the model tokenizer's `<unused>` placeholder tokens before fine-tuning."
      ],
      "metadata": {
        "id": "new_markdown_cell_for_mapping"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify special tokens in your data that are not in the base vocabulary\n",
        "unk_token_id = tokenizer.unk_token_id\n",
        "unique_chars_in_data = {char for emoji_string in dataset['emoji'] if emoji_string for char in emoji_string}\n",
        "special_tokens_in_data = {char for char in unique_chars_in_data if tokenizer.convert_tokens_to_ids(char) == unk_token_id}\n",
        "\n",
        "# Get the list of available unused tokens that are already in the vocabulary\n",
        "available_placeholders = [token for token in tokenizer.get_vocab().keys() if token.startswith(\"<unused\")]\n",
        "if len(special_tokens_in_data) > len(available_placeholders):\n",
        "    raise ValueError(\"Not enough unused tokens to map all special tokens in the dataset.\")\n",
        "\n",
        "# Create unused token mapping\n",
        "token_map = {special: unused for special, unused in zip(special_tokens_in_data, available_placeholders)}\n",
        "def replace_special_tokens(example):\n",
        "    if example['emoji']:\n",
        "        example['emoji'] = \"\".join(token_map.get(char, char) for char in example['emoji'])\n",
        "    return example\n",
        "\n",
        "print(\"Mapped {len(special_tokens_in_data)} special token(s):\", token_map)"
      ],
      "metadata": {
        "id": "new_code_cell_for_mapping"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3w3b9-O4fDz"
      },
      "source": [
        "### Recommended: Test the base model\n",
        "\n",
        "Let's first check how the base model's ability to respond to the instruction \"Translate this text to emoji\"\n",
        "\n",
        "Try testing it a few times."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "# Select a random sample from the test dataset\n",
        "rand_idx = randint(0, len(training_dataset_splits[\"test\"]) - 1)\n",
        "test_sample = training_dataset_splits[\"test\"][rand_idx]\n",
        "\n",
        "# Handle messages\n",
        "all_messages = test_sample['messages']\n",
        "user_message_content = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'user'), \"Not Found\")\n",
        "dataset_emoji = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'assistant'), \"Not Found\")\n",
        "prompt_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "    {\"role\": \"user\", \"content\": user_message_content}\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Generate the output\n",
        "output = pipe(prompt, max_new_tokens=64)\n",
        "model_output_only = output[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\nDataset text: {user_message_content}\")\n",
        "print(f\"\\nDataset emoji: {dataset_emoji}\")\n",
        "print(f\"\\nModel generated output: {model_output_only}\")"
      ],
      "metadata": {
        "id": "u8L0_INJyUok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You'll see that the base model does not generate a good emoji combination.** It might not output emoji within its response at all!\n",
        "\n",
        "That's because the Gemma 3 270M model size was designed for task specialization, which means it can improve performance for specific use cases when fine-tuned with representative examples. While the instruction-tuned model's previous training makes the task easier to learn, it's best to provide as many contextual examples in your dataset as possible.\n",
        "\n",
        "Now, you'll train the model with new data so it more reliably generates emoji."
      ],
      "metadata": {
        "id": "ph26HDJgua3W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd9fc1b"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Transformer Reinforcement Learning ([TRL](https://huggingface.co/docs/trl/index)) is a library that provides tools for training and fine-tuning LLMs using advanced techniques like QLoRA, which trains model adapters on top of a frozen quantized version of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the tuning job\n",
        "Define the training hyperparameters for the Gemma 3 base model:\n",
        "\n",
        "1. `BitsandBytesConfig` to quantize the model for memory efficiency\n",
        "2. `LoraConfig` for parameter-efficient fine-tuning\n",
        "2. `SFTConfig` for supervised fine-tuning\n",
        "\n",
        "QLoRA fine-tuning produces a set of adapters that we'll save to the Colab session storage so we can test model performance."
      ],
      "metadata": {
        "id": "-BJFoOdL0y8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTConfig\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"\n",
        "adapter_path = \"/content/myemoji-gemma-adapters\"      # Where to save your LoRA adapters\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,                                # Keep low to reduce overfitting\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\"lm_head\", \"embed_tokens\"]       # Save the lm_head and embed_tokens as you train the special tokens\n",
        ")\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=adapter_path,                          # Directory to save adapters\n",
        "    num_train_epochs=3,                               # Number of training epochs\n",
        "    per_device_train_batch_size=4,                    # Batch size per device during training\n",
        "    logging_strategy=\"epoch\",                         # Log every epoch\n",
        "    eval_strategy=\"epoch\",                            # Evaluate loss metrics every epoch\n",
        "    save_strategy=\"epoch\",                            # Save checkpoint every epoch\n",
        "    learning_rate=5e-5,                               # Learning rate\n",
        "    lr_scheduler_type=\"constant\",                     # Use constant learning rate scheduler\n",
        "    max_length=256,                                   # Max sequence length for model and packing of the dataset\n",
        "    gradient_checkpointing=False,                     # Use gradient checkpointing to save memory\n",
        "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
        "    optim=\"adamw_torch_fused\",                        # use fused adamw optimizer\n",
        "    report_to=\"tensorboard\",                          # Report metrics to tensorboard\n",
        "    weight_decay=0.01,                                # Added weight decay for regularization\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, quantization_config=bnb_config, device_map=\"auto\", attn_implementation='eager')\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Training configured\")"
      ],
      "metadata": {
        "id": "qiIj1ADc-exw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd88e798"
      },
      "source": [
        "### Start training\n",
        "\n",
        "`SFTTrainer` tokenizes the training and evaluation datasets and trains the base model using the parameters from the previous step.\n",
        "\n",
        "**This shouldn't take more than 5-10 minutes** using GPU acceleration for 1000 training examples. If your training is going slowly, check that you're using a T4 GPU in Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# Set the training and evaluation datasets\n",
        "train_dataset = training_dataset_splits['train']\n",
        "eval_dataset = training_dataset_splits['test']\n",
        "\n",
        "# Fine-tune the model and save the LoRA adapters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(adapter_path)\n",
        "\n",
        "print(f\"LoRA adapters saved to {adapter_path}\")"
      ],
      "metadata": {
        "id": "WqacJNeU9v7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LoRA adapters for each training checkpoint (epoch) will be saved in your temporary Colab session storage. You can choose which adapters to merge into the model. But first, take a look at the results."
      ],
      "metadata": {
        "id": "dDvGlb5xO34z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll8zZ3_u8Mt"
      },
      "source": [
        "### Plot training results\n",
        "To evaluate the model, you can plot the training and validation losses. Libraries like Matplotlib can then be used to visualize these values over training steps or epochs. This visualization helps in monitoring the training process and making informed decisions about hyperparameters tuning or early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPN-DTopaUIy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyIwS-orvWzd"
      },
      "source": [
        "Training loss measures the error on the data the model was trained on. Validation loss measures the error on a separate dataset the model has not seen before. Monitoring both helps detect overfitting (when the model performs well on training data but poorly on unseen data).\n",
        "\n",
        "- validation loss >> training loss: **overfitting**\n",
        "- validation loss > training loss: **some overfitting**\n",
        "- validation loss < training loss: **some underfitting**\n",
        "- validation loss << training loss: **underfitting**\n",
        "\n",
        "Overfitting is when the model can make accurate predictions on examples seen in its training but performs poorly with unseen data. If your task requires memory of specific examples, or specific emoji to be generated for a given text, overfitting can be a good thing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge the adapters\n",
        "\n",
        "Once trained you can merge the LoRA adapters with the model. You can choose which adapters to merge by specifying the training checkpoint folder, otherwise it will default to the last epoch.\n",
        "* For better task generalization, choose the most underfit checkpoint (validation loss < training loss)\n",
        "* For better memorization of specific examples, choose the most overfit (checkpoint > training loss)  \n",
        "\n"
      ],
      "metadata": {
        "id": "ILgEZvZ71Edz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "gemma_model = \"google/gemma-3-270m-it\"                            # Base model to merge from Hugging Face Hub\n",
        "adapter_path = \"/content/myemoji-gemma-adapters/\"                 # Choose which adapters to merge, otherwise defaults to latest\n",
        "merged_model_path = \"/content/myemoji-gemma-merged/\"              # Location of merged model directory\n",
        "\n",
        "# Load base model and tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "\n",
        "# Load and merge the PEFT adapters onto the base model\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model and its tokenizer\n",
        "model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "print(f\"Model merged and saved to {merged_model_path}. Final model vocabulary size: {model.config.vocab_size}\")"
      ],
      "metadata": {
        "id": "A7e5BQ9U06Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary size should be the same as the base model (262144) to ensure compatibility for different runtimes."
      ],
      "metadata": {
        "id": "jAn8LI6dxnSV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf86e31d"
      },
      "source": [
        "### Test the fine-tuned model\n",
        "\n",
        "After the training is done and you've merged the adapters to the base model, you can test different user inputs for your fine-tuned model by updating `text_to_translate`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Create Transformers inference pipeline\n",
        "merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
        "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
        "\n",
        "# Test a prompt\n",
        "text_to_translate = \"Let's dance the night away\"  #@param {type:\"string\"}\n",
        "inference_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
        "    {\"role\": \"user\", \"content\": text_to_translate}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(inference_messages, tokenize=False, add_generation_prompt=True)\n",
        "output = pipe(prompt, max_new_tokens=64)\n",
        "model_output = output[0]['generated_text']\n",
        "\n",
        "print(f\"{model_output}\")"
      ],
      "metadata": {
        "id": "28R3pRN_hai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the model output the emoji you'd expect?\n",
        "\n",
        "If you're not getting the results you want, you can try [retraining the model](#scrollTo=-BJFoOdL0y8w) using different parameters, or updating your training dataset to contain more representative examples.\n",
        "\n",
        "Once you're happy with the results, you can save your model to Hugging Face Hub for easy access."
      ],
      "metadata": {
        "id": "86qPcFbHH_kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save your model and upload to Hugging Face Hub\n",
        "**You now have a customized Gemma 3 270M model! ðŸŽ‰**\n",
        "\n",
        "You can now download the merged model (or just your LoRA adapters) to the Download folders on your local machine, or upload to a repository on Hugging Face so you easily share your model or access it later for conversion."
      ],
      "metadata": {
        "id": "H12D9g4X_peV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name your fine-tuned model\n",
        "model_name = \"myemoji\"      #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "SDenDlxGMA-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload to Hugging Face\n",
        "Upload your merged model to [Hugging Face](https://huggingface.co/) for easy sharing and future use."
      ],
      "metadata": {
        "id": "mLFJWnH3K48a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami, HfApi\n",
        "from peft import PeftModel\n",
        "\n",
        "user_info = whoami()\n",
        "username = user_info['name']\n",
        "repo_id = f\"{username}/{model_name}-gemma-3-270m-it\"\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, exist_ok=True)\n",
        "api.upload_folder(folder_path=merged_model_path, repo_id=repo_id, repo_type=\"model\")"
      ],
      "metadata": {
        "id": "VbeyDcpwi4IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8ff452"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This notebook covered how to perform QLoRA fine-tuning on Gemma 3 270M using the Transformers TRL library. Now you can use your customized model in apps!\n",
        "\n",
        "### Run your model on-device\n",
        "\n",
        "To run your fine-tuned model on-device, continue on to the conversion and quantization steps. For our web app running the model directly in the browser, you can either follow the steps to:\n",
        "\n",
        "1.  [Convert for use with MediaPipe LLM Inference API](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb)\n",
        "2.  [Convert for use with Transformers.js via ONNX Runtime](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)\n",
        "\n",
        "When converting and running inference with the model, you'll get the best performance by using the same chat template it was trained on:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Translate this text to emoji.\n",
        "\n",
        "{input}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{output}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "collapsed_sections": [
        "kc1rhhdFGwNu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}